# ü§ó Hugging Face Integration & AI Insights

## ‚úÖ Why Hugging Face?

- **100% FREE** - No credit card ever needed
- **No quotas** - Generous rate limits on free tier
- **Thousands of models** - Choose what works best
- **Easy setup** - Get API key in 1 minute
- **Always free** - Not a trial, actually free forever

---

## üîë Step 1: Get Your Hugging Face API Key (1 Minute!)

1. **Visit:** https://huggingface.co/settings/tokens
2. **Sign up** with email (or Google/GitHub)
3. **Click "New token"**
4. **Name it:** `powerpulse-api`
5. **Role:** Select "Read"
6. **Copy the token** (starts with `hf_...`)

**That's it!** No credit card, no verification, completely free.

---

## üöÄ Step 2: Add to Lambda (No Redeployment!)

### Fastest Way - Lambda Console:

1. **Go to AWS Lambda Console**
   - https://ap-south-1.console.aws.amazon.com/lambda/

2. **Open function:** `powerpulse-dev-generateInsights`

3. **Configuration ‚Üí Environment variables ‚Üí Edit**

4. **Add these:**
   ```
   Key: HUGGINGFACE_API_KEY
   Value: hf_your_actual_token_here
   
   Key: USE_AI
   Value: true
   
   Key: AI_MODEL (optional)
   Value: mistralai/Mistral-7B-Instruct-v0.2
   ```

5. **Click Save**

6. **Test immediately!** - Go to dashboard and click "Generate Insights"

---

## ü§ñ Available FREE Models

You can change the `AI_MODEL` environment variable to any of these:

### **Recommended Models:**

| Model | Speed | Quality | Best For |
|-------|-------|---------|----------|
| **`mistralai/Mistral-7B-Instruct-v0.2`** | ‚ö°‚ö° Fast | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | **Best overall (RECOMMENDED)** |
| **`microsoft/Phi-3-mini-4k-instruct`** | ‚ö°‚ö°‚ö° Very Fast | ‚≠ê‚≠ê‚≠ê‚≠ê | Quick responses |
| **`google/flan-t5-large`** | ‚ö°‚ö°‚ö° Very Fast | ‚≠ê‚≠ê‚≠ê | Structured output |
| **`meta-llama/Meta-Llama-3-8B-Instruct`** | ‚ö°‚ö° Fast | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | High quality |
| **`HuggingFaceH4/zephyr-7b-beta`** | ‚ö°‚ö° Fast | ‚≠ê‚≠ê‚≠ê‚≠ê | Good balance |

**Default model in this project:** `google/gemma-2-9b-it` (override with env `AI_MODEL`).

---

## üß™ Step 3: Test It!

1. **Open your dashboard**
2. **Click "Generate Insights"**
3. **First request may take 20-30 seconds** (model loading - only first time!)
4. **Subsequent requests: 2-5 seconds** ‚ö°

### What You'll See:

```
ü§ñ AI Insight 1
Your Air Conditioner (3-star) is consuming 5.5 kWh daily, which is 
42% of your total usage. With Indore's current pleasant 24¬∞C weather, 
you could save ‚Çπ360/month by using it 2 hours less per day.

ü§ñ AI Insight 2
Your 2-star Geyser and Fridge are less efficient. Upgrading both 
to 5-star models could reduce consumption by 2.5 kWh/day, saving 
approximately ‚Çπ450/month (‚Çπ6/kWh rate).

ü§ñ AI Insight 3
Excellent! Your usage dropped 18% yesterday. The rainy weather 
helped reduce cooling needs. Maintain this pattern by using natural 
ventilation when temperature is below 25¬∞C.

PLUS all your rule-based insights!
```

---

## üß† How AI Insights Are Generated
1) User requests `/insights` with Cognito ID token.
2) Backend loads last 30 days of readings + user profile + optional weather.
3) Rule‚Äëbased insights are generated immediately (spikes, trends, weather, cost).
4) We compute a request hash (last 7 readings + profile + model + date bucket) and check DynamoDB `InsightsCache`.
   - If cached (status=ready): return cached AI insights immediately.
   - If not cached: call Hugging Face v1 Chat Completions:
     - POST `https://router.huggingface.co/v1/chat/completions`
     - `{ model, messages: [{ role: 'user', content: <context> }], max_tokens, temperature }`
5) Parse the text into 3‚Äì5 cards and store in `InsightsCache` (TTL ~2 min) and `Insights` history.
6) Response includes metadata (type, cache, readingsCount, etc.).

## ‚è±Ô∏è Request Limiting & Caching
- Rate limiting: token‚Äëbucket (1 request/15s, max 4/min per user).
- On limit, API responds 429 and includes cached insights when available (or most recent insights).
- `InsightsCache`: avoids duplicate calls for identical inputs for ~2 minutes.

### **First Request (Cold Start):**
- **Time:** 20-30 seconds
- **Why:** Model needs to load into memory
- **Solution:** Just wait, only happens once

### **Subsequent Requests:**
- **Time:** 2-5 seconds
- **Why:** Model is already loaded
- **Great for:** Regular usage

### **If Model Goes to Sleep:**
- **After:** ~30 minutes of inactivity
- **Solution:** First request takes 20-30s again
- **Totally normal** for free tier

---

## ‚ö° Performance Notes

### **What You Get FREE:**

- **Rate Limit:** ~1000 requests/day
- **Concurrent:** 10 requests at once
- **Model Loading:** Sometimes slow (acceptable for free!)
- **Cost:** $0 forever

### **For PowerPulse:**
- **Typical usage:** 5-10 requests/user/day
- **Your app:** Can support 100+ users easily
- **Perfect for:** Student projects, demos, small apps

---

## üêõ Troubleshooting

### "Model is loading" (503 Error)

**Normal!** This happens on first request or after inactivity.

**What to do:**
- Wait 20-30 seconds
- Try again
- Model will be warm for next requests

**In logs you'll see:**
```
Hugging Face model is loading, this may take 20-30 seconds
```

### "Unauthorized" (401 Error)

**Check:**
1. API key starts with `hf_`
2. No extra spaces in key
3. Token is "Read" role (not "Write")

### "Rate limit exceeded" (429 Error)

**Unlikely but possible:**
- Wait 1 minute
- Free tier resets frequently
- Consider caching insights (generate once/day)

### Model Not Working Well?

**Try a different model:**
```
AI_MODEL = microsoft/Phi-3-mini-4k-instruct  (faster)
AI_MODEL = google/flan-t5-large             (simpler)
AI_MODEL = HuggingFaceH4/zephyr-7b-beta     (alternative)
```

---

## üîÑ Deploy with Environment Variables

If you prefer to deploy (instead of manual Lambda update):

```bash
# In Git Bash
cd /d/Coding/powerPulse/powerpulse

# Set environment variables
export HUGGINGFACE_API_KEY="hf_your_token"
export OPENWEATHER_API_KEY="your_weather_key"
export USE_AI="true"
export AI_MODEL="mistralai/Mistral-7B-Instruct-v0.2"

# Deploy
serverless deploy --stage dev
```

---

## üìä Comparison: Hugging Face vs Others

| Feature | Hugging Face | Gemini | OpenAI |
|---------|--------------|--------|---------|
| **Cost** | ‚úÖ FREE | ‚ö†Ô∏è Free but complex | ‚ùå Paid |
| **Credit Card** | ‚úÖ Not needed | ‚úÖ Not needed | ‚ùå Required |
| **Setup** | ‚úÖ 1 minute | ‚ö†Ô∏è 5 minutes | ‚ö†Ô∏è 10 minutes |
| **Cold Start** | ‚ö†Ô∏è 20-30s first time | ‚úÖ Fast always | ‚úÖ Fast always |
| **Quality** | ‚úÖ Excellent | ‚úÖ Excellent | ‚úÖ Excellent |
| **Rate Limits** | ‚úÖ Generous | ‚ö†Ô∏è Can be confusing | ‚úÖ High |
| **Best for** | ‚úÖ **Students, demos** | Production | Production |

---

## üéì For Your Project

You can say:
- ‚úÖ "Integrated Hugging Face Mistral-7B AI model"
- ‚úÖ "Zero-cost AI implementation"
- ‚úÖ "Supports multiple AI models via configuration"
- ‚úÖ "Hybrid approach: Rule-based + AI-enhanced"
- ‚úÖ "Scalable to 1000+ users on free tier"

---

## üîó Useful Links

- **Get API Token:** https://huggingface.co/settings/tokens
- **Browse Models:** https://huggingface.co/models
- **API Docs:** https://huggingface.co/docs/inference-providers
- **Inference Providers API:** https://router.huggingface.co/hf-inference
- **Migration Guide:** https://huggingface.co/docs/inference-providers/migration

---

## ‚úÖ Success Checklist

- [ ] Created Hugging Face account
- [ ] Generated API token (starts with `hf_`)
- [ ] Added `HUGGINGFACE_API_KEY` to Lambda
- [ ] Set `USE_AI = true`
- [ ] Clicked "Generate Insights"
- [ ] Waited 20-30 seconds (first time only)
- [ ] Saw AI insights with ü§ñ icon
- [ ] Subsequent requests are fast (2-5s)

---

**Ready? Get your token and add it to Lambda environment variables!** üöÄ

**No redeployment needed - just update Lambda env vars and test!**

